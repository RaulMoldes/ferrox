//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_86
.address_size 64

	// .globl	fill_random

.visible .entry fill_random(
	.param .u64 fill_random_param_0,
	.param .u32 fill_random_param_1,
	.param .u64 fill_random_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [fill_random_param_0];
	ld.param.u32 	%r2, [fill_random_param_1];
	ld.param.u64 	%rd2, [fill_random_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvt.u32.u64 	%r6, %rd2;
	add.s32 	%r7, %r1, %r6;
	add.s32 	%r8, %r7, 1;
	shl.b32 	%r9, %r8, 13;
	xor.b32  	%r10, %r9, %r8;
	shr.u32 	%r11, %r10, 17;
	xor.b32  	%r12, %r11, %r10;
	shl.b32 	%r13, %r12, 5;
	xor.b32  	%r14, %r13, %r12;
	cvt.rn.f32.u32 	%f1, %r14;
	mul.f32 	%f2, %f1, 0f2F800000;
	fma.rn.f32 	%f3, %f2, 0f40000000, 0fBF800000;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	st.global.f32 	[%rd5], %f3;

$L__BB0_2:
	ret;

}
	// .globl	fill_random_f64
.visible .entry fill_random_f64(
	.param .u64 fill_random_f64_param_0,
	.param .u32 fill_random_f64_param_1,
	.param .u64 fill_random_f64_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [fill_random_f64_param_0];
	ld.param.u32 	%r2, [fill_random_f64_param_1];
	ld.param.u64 	%rd2, [fill_random_f64_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvt.u32.u64 	%r6, %rd2;
	add.s32 	%r7, %r1, %r6;
	add.s32 	%r8, %r7, 1;
	shl.b32 	%r9, %r8, 13;
	xor.b32  	%r10, %r9, %r8;
	shr.u32 	%r11, %r10, 17;
	xor.b32  	%r12, %r11, %r10;
	shl.b32 	%r13, %r12, 5;
	xor.b32  	%r14, %r13, %r12;
	cvt.rn.f64.u32 	%fd1, %r14;
	div.rn.f64 	%fd2, %fd1, 0d41EFFFFFFFE00000;
	cvt.rn.f32.f64 	%f1, %fd2;
	fma.rn.f32 	%f2, %f1, 0f40000000, 0fBF800000;
	cvt.f64.f32 	%fd3, %f2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	st.global.f64 	[%rd5], %fd3;

$L__BB1_2:
	ret;

}
	// .globl	fill
.visible .entry fill(
	.param .u64 fill_param_0,
	.param .f32 fill_param_1,
	.param .u32 fill_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_param_0];
	ld.param.f32 	%f1, [fill_param_1];
	ld.param.u32 	%r2, [fill_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

$L__BB2_2:
	ret;

}
	// .globl	fill_f64
.visible .entry fill_f64(
	.param .u64 fill_f64_param_0,
	.param .f64 fill_f64_param_1,
	.param .u32 fill_f64_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_f64_param_0];
	ld.param.f64 	%fd1, [fill_f64_param_1];
	ld.param.u32 	%r2, [fill_f64_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f64 	[%rd4], %fd1;

$L__BB3_2:
	ret;

}

