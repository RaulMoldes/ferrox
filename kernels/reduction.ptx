//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_86
.address_size 64

	// .globl	reduce_sum_all
// _ZZ14reduce_sum_allE11shared_data has been demoted
// _ZZ18reduce_sum_all_f64E11shared_data has been demoted
// _ZZ14reduce_max_allE11shared_data has been demoted
// _ZZ18reduce_max_all_f64E11shared_data has been demoted
// _ZZ14reduce_min_allE11shared_data has been demoted
// _ZZ18reduce_min_all_f64E11shared_data has been demoted
// _ZZ15reduce_prod_allE11shared_data has been demoted
// _ZZ19reduce_prod_all_f64E11shared_data has been demoted

.visible .entry reduce_sum_all(
	.param .u64 reduce_sum_all_param_0,
	.param .u64 reduce_sum_all_param_1,
	.param .u32 reduce_sum_all_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ14reduce_sum_allE11shared_data[1024];

	ld.param.u64 	%rd8, [reduce_sum_all_param_0];
	ld.param.u64 	%rd7, [reduce_sum_all_param_1];
	ld.param.u32 	%r19, [reduce_sum_all_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f32 	%f28, 0f00000000;
	@%p1 bra 	$L__BB0_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f32 	%f28, 0f00000000;
	@%p2 bra 	$L__BB0_4;

	mul.wide.s32 	%rd9, %r35, 4;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 4;

$L__BB0_3:
	.pragma "nounroll";
	ld.global.f32 	%f12, [%rd18];
	add.f32 	%f28, %f28, %f12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB0_7;

	mul.wide.s32 	%rd6, %r5, 4;

$L__BB0_6:
	mul.wide.s32 	%rd10, %r35, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f13, [%rd11];
	add.f32 	%f14, %f28, %f13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f32 	%f15, [%rd12];
	add.f32 	%f16, %f14, %f15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f32 	%f17, [%rd13];
	add.f32 	%f18, %f16, %f17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f32 	%f19, [%rd14];
	add.f32 	%f28, %f18, %f19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	shl.b32 	%r29, %r3, 2;
	mov.u32 	%r30, _ZZ14reduce_sum_allE11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f32 	[%r15], %f28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB0_12;

$L__BB0_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB0_11;

	shl.b32 	%r31, %r37, 2;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f32 	%f20, [%r15];
	ld.shared.f32 	%f21, [%r32];
	add.f32 	%f22, %f21, %f20;
	st.shared.f32 	[%r15], %f22;

$L__BB0_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB0_9;

$L__BB0_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB0_14;

	ld.shared.f32 	%f23, [_ZZ14reduce_sum_allE11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f23;

$L__BB0_14:
	ret;

}
	// .globl	reduce_sum_all_f64
.visible .entry reduce_sum_all_f64(
	.param .u64 reduce_sum_all_f64_param_0,
	.param .u64 reduce_sum_all_f64_param_1,
	.param .u32 reduce_sum_all_f64_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 8 .b8 _ZZ18reduce_sum_all_f64E11shared_data[2048];

	ld.param.u64 	%rd8, [reduce_sum_all_f64_param_0];
	ld.param.u64 	%rd7, [reduce_sum_all_f64_param_1];
	ld.param.u32 	%r19, [reduce_sum_all_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f64 	%fd28, 0d0000000000000000;
	@%p1 bra 	$L__BB1_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	@%p2 bra 	$L__BB1_4;

	mul.wide.s32 	%rd9, %r35, 8;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 8;

$L__BB1_3:
	.pragma "nounroll";
	ld.global.f64 	%fd12, [%rd18];
	add.f64 	%fd28, %fd28, %fd12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB1_7;

	mul.wide.s32 	%rd6, %r5, 8;

$L__BB1_6:
	mul.wide.s32 	%rd10, %r35, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd13, [%rd11];
	add.f64 	%fd14, %fd28, %fd13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f64 	%fd15, [%rd12];
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f64 	%fd17, [%rd13];
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f64 	%fd19, [%rd14];
	add.f64 	%fd28, %fd18, %fd19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB1_6;

$L__BB1_7:
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ18reduce_sum_all_f64E11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f64 	[%r15], %fd28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB1_12;

$L__BB1_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB1_11;

	shl.b32 	%r31, %r37, 3;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f64 	%fd20, [%r15];
	ld.shared.f64 	%fd21, [%r32];
	add.f64 	%fd22, %fd21, %fd20;
	st.shared.f64 	[%r15], %fd22;

$L__BB1_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB1_9;

$L__BB1_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB1_14;

	ld.shared.f64 	%fd23, [_ZZ18reduce_sum_all_f64E11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd23;

$L__BB1_14:
	ret;

}
	// .globl	reduce_sum_axes
.visible .entry reduce_sum_axes(
	.param .u64 reduce_sum_axes_param_0,
	.param .u64 reduce_sum_axes_param_1,
	.param .u32 reduce_sum_axes_param_2,
	.param .u32 reduce_sum_axes_param_3,
	.param .u32 reduce_sum_axes_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_sum_axes_param_0];
	ld.param.u64 	%rd3, [reduce_sum_axes_param_1];
	ld.param.u32 	%r19, [reduce_sum_axes_param_2];
	ld.param.u32 	%r17, [reduce_sum_axes_param_3];
	ld.param.u32 	%r18, [reduce_sum_axes_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB2_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f32 	%f24, 0f00000000;
	@%p4 bra 	$L__BB2_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f32 	%f24, 0f00000000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB2_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 4;

$L__BB2_4:
	mul.wide.s32 	%rd5, %r27, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f12, [%rd6];
	add.f32 	%f13, %f24, %f12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f32 	%f14, [%rd7];
	add.f32 	%f15, %f13, %f14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f32 	%f16, [%rd8];
	add.f32 	%f17, %f15, %f16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f32 	%f18, [%rd9];
	add.f32 	%f24, %f17, %f18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB2_4;

$L__BB2_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB2_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB2_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f19, [%rd11];
	add.f32 	%f24, %f24, %f19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB2_7;

$L__BB2_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f24;

$L__BB2_9:
	ret;

}
	// .globl	reduce_sum_axes_f64
.visible .entry reduce_sum_axes_f64(
	.param .u64 reduce_sum_axes_f64_param_0,
	.param .u64 reduce_sum_axes_f64_param_1,
	.param .u32 reduce_sum_axes_f64_param_2,
	.param .u32 reduce_sum_axes_f64_param_3,
	.param .u32 reduce_sum_axes_f64_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_sum_axes_f64_param_0];
	ld.param.u64 	%rd3, [reduce_sum_axes_f64_param_1];
	ld.param.u32 	%r19, [reduce_sum_axes_f64_param_2];
	ld.param.u32 	%r17, [reduce_sum_axes_f64_param_3];
	ld.param.u32 	%r18, [reduce_sum_axes_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f64 	%fd24, 0d0000000000000000;
	@%p4 bra 	$L__BB3_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f64 	%fd24, 0d0000000000000000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB3_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 8;

$L__BB3_4:
	mul.wide.s32 	%rd5, %r27, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd12, [%rd6];
	add.f64 	%fd13, %fd24, %fd12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f64 	%fd14, [%rd7];
	add.f64 	%fd15, %fd13, %fd14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f64 	%fd16, [%rd8];
	add.f64 	%fd17, %fd15, %fd16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f64 	%fd18, [%rd9];
	add.f64 	%fd24, %fd17, %fd18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB3_4;

$L__BB3_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB3_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB3_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd19, [%rd11];
	add.f64 	%fd24, %fd24, %fd19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB3_7;

$L__BB3_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd24;

$L__BB3_9:
	ret;

}
	// .globl	reduce_max_all
.visible .entry reduce_max_all(
	.param .u64 reduce_max_all_param_0,
	.param .u64 reduce_max_all_param_1,
	.param .u32 reduce_max_all_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ14reduce_max_allE11shared_data[1024];

	ld.param.u64 	%rd8, [reduce_max_all_param_0];
	ld.param.u64 	%rd7, [reduce_max_all_param_1];
	ld.param.u32 	%r19, [reduce_max_all_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f32 	%f28, 0fFF800000;
	@%p1 bra 	$L__BB4_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f32 	%f28, 0fFF800000;
	@%p2 bra 	$L__BB4_4;

	mul.wide.s32 	%rd9, %r35, 4;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 4;

$L__BB4_3:
	.pragma "nounroll";
	ld.global.f32 	%f12, [%rd18];
	max.f32 	%f28, %f28, %f12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB4_3;

$L__BB4_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB4_7;

	mul.wide.s32 	%rd6, %r5, 4;

$L__BB4_6:
	mul.wide.s32 	%rd10, %r35, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f13, [%rd11];
	max.f32 	%f14, %f28, %f13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f32 	%f15, [%rd12];
	max.f32 	%f16, %f14, %f15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f32 	%f17, [%rd13];
	max.f32 	%f18, %f16, %f17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f32 	%f19, [%rd14];
	max.f32 	%f28, %f18, %f19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB4_6;

$L__BB4_7:
	shl.b32 	%r29, %r3, 2;
	mov.u32 	%r30, _ZZ14reduce_max_allE11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f32 	[%r15], %f28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB4_12;

$L__BB4_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB4_11;

	ld.shared.f32 	%f20, [%r15];
	shl.b32 	%r31, %r37, 2;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f32 	%f21, [%r32];
	max.f32 	%f22, %f20, %f21;
	st.shared.f32 	[%r15], %f22;

$L__BB4_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB4_9;

$L__BB4_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB4_14;

	ld.shared.f32 	%f23, [_ZZ14reduce_max_allE11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f23;

$L__BB4_14:
	ret;

}
	// .globl	reduce_max_all_f64
.visible .entry reduce_max_all_f64(
	.param .u64 reduce_max_all_f64_param_0,
	.param .u64 reduce_max_all_f64_param_1,
	.param .u32 reduce_max_all_f64_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 8 .b8 _ZZ18reduce_max_all_f64E11shared_data[2048];

	ld.param.u64 	%rd8, [reduce_max_all_f64_param_0];
	ld.param.u64 	%rd7, [reduce_max_all_f64_param_1];
	ld.param.u32 	%r19, [reduce_max_all_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f64 	%fd28, 0dFFF0000000000000;
	@%p1 bra 	$L__BB5_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f64 	%fd28, 0dFFF0000000000000;
	@%p2 bra 	$L__BB5_4;

	mul.wide.s32 	%rd9, %r35, 8;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 8;

$L__BB5_3:
	.pragma "nounroll";
	ld.global.f64 	%fd12, [%rd18];
	max.f64 	%fd28, %fd28, %fd12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB5_3;

$L__BB5_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB5_7;

	mul.wide.s32 	%rd6, %r5, 8;

$L__BB5_6:
	mul.wide.s32 	%rd10, %r35, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd13, [%rd11];
	max.f64 	%fd14, %fd28, %fd13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f64 	%fd15, [%rd12];
	max.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f64 	%fd17, [%rd13];
	max.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f64 	%fd19, [%rd14];
	max.f64 	%fd28, %fd18, %fd19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB5_6;

$L__BB5_7:
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ18reduce_max_all_f64E11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f64 	[%r15], %fd28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB5_12;

$L__BB5_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB5_11;

	ld.shared.f64 	%fd20, [%r15];
	shl.b32 	%r31, %r37, 3;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f64 	%fd21, [%r32];
	max.f64 	%fd22, %fd20, %fd21;
	st.shared.f64 	[%r15], %fd22;

$L__BB5_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB5_9;

$L__BB5_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB5_14;

	ld.shared.f64 	%fd23, [_ZZ18reduce_max_all_f64E11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd23;

$L__BB5_14:
	ret;

}
	// .globl	reduce_max_axes
.visible .entry reduce_max_axes(
	.param .u64 reduce_max_axes_param_0,
	.param .u64 reduce_max_axes_param_1,
	.param .u32 reduce_max_axes_param_2,
	.param .u32 reduce_max_axes_param_3,
	.param .u32 reduce_max_axes_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_max_axes_param_0];
	ld.param.u64 	%rd3, [reduce_max_axes_param_1];
	ld.param.u32 	%r19, [reduce_max_axes_param_2];
	ld.param.u32 	%r17, [reduce_max_axes_param_3];
	ld.param.u32 	%r18, [reduce_max_axes_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB6_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f32 	%f24, 0fFF800000;
	@%p4 bra 	$L__BB6_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f32 	%f24, 0fFF800000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB6_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 4;

$L__BB6_4:
	mul.wide.s32 	%rd5, %r27, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f12, [%rd6];
	max.f32 	%f13, %f24, %f12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f32 	%f14, [%rd7];
	max.f32 	%f15, %f13, %f14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f32 	%f16, [%rd8];
	max.f32 	%f17, %f15, %f16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f32 	%f18, [%rd9];
	max.f32 	%f24, %f17, %f18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB6_4;

$L__BB6_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB6_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB6_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f19, [%rd11];
	max.f32 	%f24, %f24, %f19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB6_7;

$L__BB6_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f24;

$L__BB6_9:
	ret;

}
	// .globl	reduce_max_axes_f64
.visible .entry reduce_max_axes_f64(
	.param .u64 reduce_max_axes_f64_param_0,
	.param .u64 reduce_max_axes_f64_param_1,
	.param .u32 reduce_max_axes_f64_param_2,
	.param .u32 reduce_max_axes_f64_param_3,
	.param .u32 reduce_max_axes_f64_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_max_axes_f64_param_0];
	ld.param.u64 	%rd3, [reduce_max_axes_f64_param_1];
	ld.param.u32 	%r19, [reduce_max_axes_f64_param_2];
	ld.param.u32 	%r17, [reduce_max_axes_f64_param_3];
	ld.param.u32 	%r18, [reduce_max_axes_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB7_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f64 	%fd24, 0dFFF0000000000000;
	@%p4 bra 	$L__BB7_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f64 	%fd24, 0dFFF0000000000000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB7_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 8;

$L__BB7_4:
	mul.wide.s32 	%rd5, %r27, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd12, [%rd6];
	max.f64 	%fd13, %fd24, %fd12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f64 	%fd14, [%rd7];
	max.f64 	%fd15, %fd13, %fd14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f64 	%fd16, [%rd8];
	max.f64 	%fd17, %fd15, %fd16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f64 	%fd18, [%rd9];
	max.f64 	%fd24, %fd17, %fd18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB7_4;

$L__BB7_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB7_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB7_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd19, [%rd11];
	max.f64 	%fd24, %fd24, %fd19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB7_7;

$L__BB7_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd24;

$L__BB7_9:
	ret;

}
	// .globl	reduce_min_all
.visible .entry reduce_min_all(
	.param .u64 reduce_min_all_param_0,
	.param .u64 reduce_min_all_param_1,
	.param .u32 reduce_min_all_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ14reduce_min_allE11shared_data[1024];

	ld.param.u64 	%rd8, [reduce_min_all_param_0];
	ld.param.u64 	%rd7, [reduce_min_all_param_1];
	ld.param.u32 	%r19, [reduce_min_all_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f32 	%f28, 0f7F800000;
	@%p1 bra 	$L__BB8_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f32 	%f28, 0f7F800000;
	@%p2 bra 	$L__BB8_4;

	mul.wide.s32 	%rd9, %r35, 4;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 4;

$L__BB8_3:
	.pragma "nounroll";
	ld.global.f32 	%f12, [%rd18];
	min.f32 	%f28, %f28, %f12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB8_3;

$L__BB8_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB8_7;

	mul.wide.s32 	%rd6, %r5, 4;

$L__BB8_6:
	mul.wide.s32 	%rd10, %r35, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f13, [%rd11];
	min.f32 	%f14, %f28, %f13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f32 	%f15, [%rd12];
	min.f32 	%f16, %f14, %f15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f32 	%f17, [%rd13];
	min.f32 	%f18, %f16, %f17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f32 	%f19, [%rd14];
	min.f32 	%f28, %f18, %f19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB8_6;

$L__BB8_7:
	shl.b32 	%r29, %r3, 2;
	mov.u32 	%r30, _ZZ14reduce_min_allE11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f32 	[%r15], %f28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB8_12;

$L__BB8_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB8_11;

	ld.shared.f32 	%f20, [%r15];
	shl.b32 	%r31, %r37, 2;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f32 	%f21, [%r32];
	min.f32 	%f22, %f20, %f21;
	st.shared.f32 	[%r15], %f22;

$L__BB8_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB8_9;

$L__BB8_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB8_14;

	ld.shared.f32 	%f23, [_ZZ14reduce_min_allE11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f23;

$L__BB8_14:
	ret;

}
	// .globl	reduce_min_all_f64
.visible .entry reduce_min_all_f64(
	.param .u64 reduce_min_all_f64_param_0,
	.param .u64 reduce_min_all_f64_param_1,
	.param .u32 reduce_min_all_f64_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 8 .b8 _ZZ18reduce_min_all_f64E11shared_data[2048];

	ld.param.u64 	%rd8, [reduce_min_all_f64_param_0];
	ld.param.u64 	%rd7, [reduce_min_all_f64_param_1];
	ld.param.u32 	%r19, [reduce_min_all_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f64 	%fd28, 0d7FF0000000000000;
	@%p1 bra 	$L__BB9_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f64 	%fd28, 0d7FF0000000000000;
	@%p2 bra 	$L__BB9_4;

	mul.wide.s32 	%rd9, %r35, 8;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 8;

$L__BB9_3:
	.pragma "nounroll";
	ld.global.f64 	%fd12, [%rd18];
	min.f64 	%fd28, %fd28, %fd12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB9_3;

$L__BB9_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB9_7;

	mul.wide.s32 	%rd6, %r5, 8;

$L__BB9_6:
	mul.wide.s32 	%rd10, %r35, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd13, [%rd11];
	min.f64 	%fd14, %fd28, %fd13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f64 	%fd15, [%rd12];
	min.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f64 	%fd17, [%rd13];
	min.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f64 	%fd19, [%rd14];
	min.f64 	%fd28, %fd18, %fd19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB9_6;

$L__BB9_7:
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ18reduce_min_all_f64E11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f64 	[%r15], %fd28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB9_12;

$L__BB9_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB9_11;

	ld.shared.f64 	%fd20, [%r15];
	shl.b32 	%r31, %r37, 3;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f64 	%fd21, [%r32];
	min.f64 	%fd22, %fd20, %fd21;
	st.shared.f64 	[%r15], %fd22;

$L__BB9_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB9_9;

$L__BB9_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB9_14;

	ld.shared.f64 	%fd23, [_ZZ18reduce_min_all_f64E11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd23;

$L__BB9_14:
	ret;

}
	// .globl	reduce_min_axes
.visible .entry reduce_min_axes(
	.param .u64 reduce_min_axes_param_0,
	.param .u64 reduce_min_axes_param_1,
	.param .u32 reduce_min_axes_param_2,
	.param .u32 reduce_min_axes_param_3,
	.param .u32 reduce_min_axes_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_min_axes_param_0];
	ld.param.u64 	%rd3, [reduce_min_axes_param_1];
	ld.param.u32 	%r19, [reduce_min_axes_param_2];
	ld.param.u32 	%r17, [reduce_min_axes_param_3];
	ld.param.u32 	%r18, [reduce_min_axes_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB10_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f32 	%f24, 0f7F800000;
	@%p4 bra 	$L__BB10_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f32 	%f24, 0f7F800000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB10_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 4;

$L__BB10_4:
	mul.wide.s32 	%rd5, %r27, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f12, [%rd6];
	min.f32 	%f13, %f24, %f12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f32 	%f14, [%rd7];
	min.f32 	%f15, %f13, %f14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f32 	%f16, [%rd8];
	min.f32 	%f17, %f15, %f16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f32 	%f18, [%rd9];
	min.f32 	%f24, %f17, %f18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB10_4;

$L__BB10_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB10_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB10_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f19, [%rd11];
	min.f32 	%f24, %f24, %f19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB10_7;

$L__BB10_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f24;

$L__BB10_9:
	ret;

}
	// .globl	reduce_min_axes_f64
.visible .entry reduce_min_axes_f64(
	.param .u64 reduce_min_axes_f64_param_0,
	.param .u64 reduce_min_axes_f64_param_1,
	.param .u32 reduce_min_axes_f64_param_2,
	.param .u32 reduce_min_axes_f64_param_3,
	.param .u32 reduce_min_axes_f64_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_min_axes_f64_param_0];
	ld.param.u64 	%rd3, [reduce_min_axes_f64_param_1];
	ld.param.u32 	%r19, [reduce_min_axes_f64_param_2];
	ld.param.u32 	%r17, [reduce_min_axes_f64_param_3];
	ld.param.u32 	%r18, [reduce_min_axes_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB11_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f64 	%fd24, 0d7FF0000000000000;
	@%p4 bra 	$L__BB11_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f64 	%fd24, 0d7FF0000000000000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB11_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 8;

$L__BB11_4:
	mul.wide.s32 	%rd5, %r27, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd12, [%rd6];
	min.f64 	%fd13, %fd24, %fd12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f64 	%fd14, [%rd7];
	min.f64 	%fd15, %fd13, %fd14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f64 	%fd16, [%rd8];
	min.f64 	%fd17, %fd15, %fd16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f64 	%fd18, [%rd9];
	min.f64 	%fd24, %fd17, %fd18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB11_4;

$L__BB11_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB11_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB11_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd19, [%rd11];
	min.f64 	%fd24, %fd24, %fd19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB11_7;

$L__BB11_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd24;

$L__BB11_9:
	ret;

}
	// .globl	reduce_prod_all
.visible .entry reduce_prod_all(
	.param .u64 reduce_prod_all_param_0,
	.param .u64 reduce_prod_all_param_1,
	.param .u32 reduce_prod_all_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ15reduce_prod_allE11shared_data[1024];

	ld.param.u64 	%rd8, [reduce_prod_all_param_0];
	ld.param.u64 	%rd7, [reduce_prod_all_param_1];
	ld.param.u32 	%r19, [reduce_prod_all_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f32 	%f28, 0f3F800000;
	@%p1 bra 	$L__BB12_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f32 	%f28, 0f3F800000;
	@%p2 bra 	$L__BB12_4;

	mul.wide.s32 	%rd9, %r35, 4;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 4;

$L__BB12_3:
	.pragma "nounroll";
	ld.global.f32 	%f12, [%rd18];
	mul.f32 	%f28, %f28, %f12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB12_3;

$L__BB12_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB12_7;

	mul.wide.s32 	%rd6, %r5, 4;

$L__BB12_6:
	mul.wide.s32 	%rd10, %r35, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f13, [%rd11];
	mul.f32 	%f14, %f28, %f13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f32 	%f15, [%rd12];
	mul.f32 	%f16, %f14, %f15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f32 	%f17, [%rd13];
	mul.f32 	%f18, %f16, %f17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f32 	%f19, [%rd14];
	mul.f32 	%f28, %f18, %f19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB12_6;

$L__BB12_7:
	shl.b32 	%r29, %r3, 2;
	mov.u32 	%r30, _ZZ15reduce_prod_allE11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f32 	[%r15], %f28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB12_12;

$L__BB12_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB12_11;

	shl.b32 	%r31, %r37, 2;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f32 	%f20, [%r15];
	ld.shared.f32 	%f21, [%r32];
	mul.f32 	%f22, %f21, %f20;
	st.shared.f32 	[%r15], %f22;

$L__BB12_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB12_9;

$L__BB12_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB12_14;

	ld.shared.f32 	%f23, [_ZZ15reduce_prod_allE11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f23;

$L__BB12_14:
	ret;

}
	// .globl	reduce_prod_all_f64
.visible .entry reduce_prod_all_f64(
	.param .u64 reduce_prod_all_f64_param_0,
	.param .u64 reduce_prod_all_f64_param_1,
	.param .u32 reduce_prod_all_f64_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 8 .b8 _ZZ19reduce_prod_all_f64E11shared_data[2048];

	ld.param.u64 	%rd8, [reduce_prod_all_f64_param_0];
	ld.param.u64 	%rd7, [reduce_prod_all_f64_param_1];
	ld.param.u32 	%r19, [reduce_prod_all_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r35, %r2, %r1, %r3;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	setp.ge.s32 	%p1, %r35, %r19;
	mov.f64 	%fd28, 0d3FF0000000000000;
	@%p1 bra 	$L__BB13_7;

	add.s32 	%r21, %r5, %r19;
	add.s32 	%r22, %r35, %r5;
	not.b32 	%r23, %r22;
	add.s32 	%r24, %r21, %r23;
	div.u32 	%r6, %r24, %r5;
	add.s32 	%r25, %r6, 1;
	and.b32  	%r34, %r25, 3;
	setp.eq.s32 	%p2, %r34, 0;
	mov.f64 	%fd28, 0d3FF0000000000000;
	@%p2 bra 	$L__BB13_4;

	mul.wide.s32 	%rd9, %r35, 8;
	add.s64 	%rd18, %rd1, %rd9;
	mul.wide.s32 	%rd3, %r5, 8;

$L__BB13_3:
	.pragma "nounroll";
	ld.global.f64 	%fd12, [%rd18];
	mul.f64 	%fd28, %fd28, %fd12;
	add.s32 	%r35, %r35, %r5;
	add.s64 	%rd18, %rd18, %rd3;
	add.s32 	%r34, %r34, -1;
	setp.ne.s32 	%p3, %r34, 0;
	@%p3 bra 	$L__BB13_3;

$L__BB13_4:
	setp.lt.u32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB13_7;

	mul.wide.s32 	%rd6, %r5, 8;

$L__BB13_6:
	mul.wide.s32 	%rd10, %r35, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd13, [%rd11];
	mul.f64 	%fd14, %fd28, %fd13;
	add.s64 	%rd12, %rd11, %rd6;
	ld.global.f64 	%fd15, [%rd12];
	mul.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r26, %r35, %r5;
	add.s32 	%r27, %r26, %r5;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f64 	%fd17, [%rd13];
	mul.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r28, %r27, %r5;
	add.s64 	%rd14, %rd13, %rd6;
	ld.global.f64 	%fd19, [%rd14];
	mul.f64 	%fd28, %fd18, %fd19;
	add.s32 	%r35, %r28, %r5;
	setp.lt.s32 	%p5, %r35, %r19;
	@%p5 bra 	$L__BB13_6;

$L__BB13_7:
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ19reduce_prod_all_f64E11shared_data;
	add.s32 	%r15, %r30, %r29;
	st.shared.f64 	[%r15], %fd28;
	bar.sync 	0;
	shr.u32 	%r37, %r1, 1;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB13_12;

$L__BB13_9:
	setp.ge.s32 	%p7, %r3, %r37;
	@%p7 bra 	$L__BB13_11;

	shl.b32 	%r31, %r37, 3;
	add.s32 	%r32, %r15, %r31;
	ld.shared.f64 	%fd20, [%r15];
	ld.shared.f64 	%fd21, [%r32];
	mul.f64 	%fd22, %fd21, %fd20;
	st.shared.f64 	[%r15], %fd22;

$L__BB13_11:
	bar.sync 	0;
	shr.s32 	%r18, %r37, 1;
	setp.gt.s32 	%p8, %r37, 1;
	mov.u32 	%r37, %r18;
	@%p8 bra 	$L__BB13_9;

$L__BB13_12:
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB13_14;

	ld.shared.f64 	%fd23, [_ZZ19reduce_prod_all_f64E11shared_data];
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.u32 	%rd16, %r2, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd23;

$L__BB13_14:
	ret;

}
	// .globl	reduce_prod_axes
.visible .entry reduce_prod_axes(
	.param .u64 reduce_prod_axes_param_0,
	.param .u64 reduce_prod_axes_param_1,
	.param .u32 reduce_prod_axes_param_2,
	.param .u32 reduce_prod_axes_param_3,
	.param .u32 reduce_prod_axes_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_prod_axes_param_0];
	ld.param.u64 	%rd3, [reduce_prod_axes_param_1];
	ld.param.u32 	%r19, [reduce_prod_axes_param_2];
	ld.param.u32 	%r17, [reduce_prod_axes_param_3];
	ld.param.u32 	%r18, [reduce_prod_axes_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB14_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f32 	%f24, 0f3F800000;
	@%p4 bra 	$L__BB14_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f32 	%f24, 0f3F800000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB14_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 4;

$L__BB14_4:
	mul.wide.s32 	%rd5, %r27, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f12, [%rd6];
	mul.f32 	%f13, %f24, %f12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f32 	%f14, [%rd7];
	mul.f32 	%f15, %f13, %f14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f32 	%f16, [%rd8];
	mul.f32 	%f17, %f15, %f16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f32 	%f18, [%rd9];
	mul.f32 	%f24, %f17, %f18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB14_4;

$L__BB14_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB14_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB14_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f19, [%rd11];
	mul.f32 	%f24, %f24, %f19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB14_7;

$L__BB14_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f24;

$L__BB14_9:
	ret;

}
	// .globl	reduce_prod_axes_f64
.visible .entry reduce_prod_axes_f64(
	.param .u64 reduce_prod_axes_f64_param_0,
	.param .u64 reduce_prod_axes_f64_param_1,
	.param .u32 reduce_prod_axes_f64_param_2,
	.param .u32 reduce_prod_axes_f64_param_3,
	.param .u32 reduce_prod_axes_f64_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_prod_axes_f64_param_0];
	ld.param.u64 	%rd3, [reduce_prod_axes_f64_param_1];
	ld.param.u32 	%r19, [reduce_prod_axes_f64_param_2];
	ld.param.u32 	%r17, [reduce_prod_axes_f64_param_3];
	ld.param.u32 	%r18, [reduce_prod_axes_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r19;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB15_9;

	setp.lt.s32 	%p4, %r17, 1;
	mov.f64 	%fd24, 0d3FF0000000000000;
	@%p4 bra 	$L__BB15_8;

	add.s32 	%r21, %r17, -1;
	and.b32  	%r31, %r17, 3;
	setp.lt.u32 	%p5, %r21, 3;
	mov.f64 	%fd24, 0d3FF0000000000000;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB15_5;

	shl.b32 	%r4, %r18, 2;
	mul.lo.s32 	%r23, %r1, %r18;
	mad.lo.s32 	%r27, %r23, %r17, %r2;
	sub.s32 	%r6, %r31, %r17;
	mul.wide.s32 	%rd2, %r18, 8;

$L__BB15_4:
	mul.wide.s32 	%rd5, %r27, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd12, [%rd6];
	mul.f64 	%fd13, %fd24, %fd12;
	add.s64 	%rd7, %rd6, %rd2;
	ld.global.f64 	%fd14, [%rd7];
	mul.f64 	%fd15, %fd13, %fd14;
	add.s64 	%rd8, %rd7, %rd2;
	ld.global.f64 	%fd16, [%rd8];
	mul.f64 	%fd17, %fd15, %fd16;
	add.s64 	%rd9, %rd8, %rd2;
	ld.global.f64 	%fd18, [%rd9];
	mul.f64 	%fd24, %fd17, %fd18;
	add.s32 	%r27, %r27, %r4;
	add.s32 	%r29, %r29, 4;
	add.s32 	%r24, %r6, %r29;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB15_4;

$L__BB15_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB15_8;

	mad.lo.s32 	%r25, %r1, %r17, %r29;
	mad.lo.s32 	%r30, %r18, %r25, %r2;

$L__BB15_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r30, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f64 	%fd19, [%rd11];
	mul.f64 	%fd24, %fd24, %fd19;
	add.s32 	%r30, %r30, %r18;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB15_7;

$L__BB15_8:
	mad.lo.s32 	%r26, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r26, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd24;

$L__BB15_9:
	ret;

}

