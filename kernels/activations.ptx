//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_86
.address_size 64

	// .globl	relu

.visible .entry relu(
	.param .u64 relu_param_0,
	.param .u64 relu_param_1,
	.param .u32 relu_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [relu_param_0];
	ld.param.u64 	%rd2, [relu_param_1];
	ld.param.u32 	%r2, [relu_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mov.f32 	%f2, 0f00000000;
	max.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

$L__BB0_2:
	ret;

}
	// .globl	sigmoid
.visible .entry sigmoid(
	.param .u64 sigmoid_param_0,
	.param .u64 sigmoid_param_1,
	.param .u32 sigmoid_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [sigmoid_param_0];
	ld.param.u64 	%rd2, [sigmoid_param_1];
	ld.param.u32 	%r2, [sigmoid_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	neg.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3F000000;
	mov.f32 	%f4, 0f3BBB989D;
	fma.rn.f32 	%f5, %f2, %f4, %f3;
	mov.f32 	%f6, 0f3FB8AA3B;
	mov.f32 	%f7, 0f437C0000;
	cvt.sat.f32.f32 	%f8, %f5;
	mov.f32 	%f9, 0f4B400001;
	fma.rm.f32 	%f10, %f8, %f7, %f9;
	add.f32 	%f11, %f10, 0fCB40007F;
	neg.f32 	%f12, %f11;
	fma.rn.f32 	%f13, %f2, %f6, %f12;
	mov.f32 	%f14, 0f32A57060;
	fma.rn.f32 	%f15, %f2, %f14, %f13;
	mov.b32 	%r6, %f10;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f16, %r7;
	ex2.approx.ftz.f32 	%f17, %f15;
	fma.rn.f32 	%f18, %f17, %f16, 0f3F800000;
	rcp.rn.f32 	%f19, %f18;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f19;

$L__BB1_2:
	ret;

}
	// .globl	hyperbolic_tangent
.visible .entry hyperbolic_tangent(
	.param .u64 hyperbolic_tangent_param_0,
	.param .u64 hyperbolic_tangent_param_1,
	.param .u32 hyperbolic_tangent_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [hyperbolic_tangent_param_0];
	ld.param.u64 	%rd3, [hyperbolic_tangent_param_1];
	ld.param.u32 	%r2, [hyperbolic_tangent_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	abs.f32 	%f2, %f1;
	setp.ltu.f32 	%p2, %f2, 0f3F19999A;
	@%p2 bra 	$L__BB2_3;
	bra.uni 	$L__BB2_2;

$L__BB2_3:
	mul.f32 	%f14, %f1, %f1;
	mov.f32 	%f15, 0fBD563CAE;
	mov.f32 	%f16, 0f3C80F082;
	fma.rn.f32 	%f17, %f16, %f14, %f15;
	mov.f32 	%f18, 0f3E085941;
	fma.rn.f32 	%f19, %f17, %f14, %f18;
	mov.f32 	%f20, 0fBEAAA9ED;
	fma.rn.f32 	%f21, %f19, %f14, %f20;
	mov.f32 	%f22, 0f00000000;
	fma.rn.f32 	%f23, %f21, %f14, %f22;
	fma.rn.f32 	%f24, %f23, %f1, %f1;
	bra.uni 	$L__BB2_4;

$L__BB2_2:
	mul.f32 	%f6, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f7, %f6;
	add.f32 	%f8, %f7, 0f3F800000;
	mov.f32 	%f9, 0f3F800000;
	rcp.approx.ftz.f32 	%f10, %f8;
	mov.f32 	%f11, 0fC0000000;
	fma.rn.f32 	%f12, %f10, %f11, %f9;
	setp.ge.f32 	%p3, %f2, 0f41102CB4;
	selp.f32 	%f13, 0f3F800000, %f12, %p3;
	mov.b32 	%r6, %f13;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f24, %r9;

$L__BB2_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f24;

$L__BB2_5:
	ret;

}
	// .globl	relu_f64
.visible .entry relu_f64(
	.param .u64 relu_f64_param_0,
	.param .u64 relu_f64_param_1,
	.param .u32 relu_f64_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [relu_f64_param_0];
	ld.param.u64 	%rd2, [relu_f64_param_1];
	ld.param.u32 	%r2, [relu_f64_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	mov.f64 	%fd2, 0d0000000000000000;
	max.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

$L__BB3_2:
	ret;

}
	// .globl	sigmoid_f64
.visible .entry sigmoid_f64(
	.param .u64 sigmoid_f64_param_0,
	.param .u64 sigmoid_f64_param_1,
	.param .u32 sigmoid_f64_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<45>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [sigmoid_f64_param_0];
	ld.param.u64 	%rd3, [sigmoid_f64_param_1];
	ld.param.u32 	%r5, [sigmoid_f64_param_2];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r5;
	@%p1 bra 	$L__BB4_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	neg.f64 	%fd6, %fd1;
	mov.f64 	%fd7, 0d4338000000000000;
	mov.f64 	%fd8, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd9, %fd6, %fd8, %fd7;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd9;
	}
	mov.f64 	%fd10, 0dC338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	mov.f64 	%fd12, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd6;
	mov.f64 	%fd14, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd15, %fd11, %fd14, %fd13;
	mov.f64 	%fd16, 0d3E928AF3FCA213EA;
	mov.f64 	%fd17, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd18, %fd17, %fd15, %fd16;
	mov.f64 	%fd19, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd20, %fd18, %fd15, %fd19;
	mov.f64 	%fd21, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd22, %fd20, %fd15, %fd21;
	mov.f64 	%fd23, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd24, %fd22, %fd15, %fd23;
	mov.f64 	%fd25, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd26, %fd24, %fd15, %fd25;
	mov.f64 	%fd27, 0d3F81111111122322;
	fma.rn.f64 	%fd28, %fd26, %fd15, %fd27;
	mov.f64 	%fd29, 0d3FA55555555502A1;
	fma.rn.f64 	%fd30, %fd28, %fd15, %fd29;
	mov.f64 	%fd31, 0d3FC5555555555511;
	fma.rn.f64 	%fd32, %fd30, %fd15, %fd31;
	mov.f64 	%fd33, 0d3FE000000000000B;
	fma.rn.f64 	%fd34, %fd32, %fd15, %fd33;
	mov.f64 	%fd35, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd34, %fd15, %fd35;
	fma.rn.f64 	%fd37, %fd36, %fd15, %fd35;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd37;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd37;
	}
	shl.b32 	%r9, %r2, 20;
	add.s32 	%r10, %r4, %r9;
	mov.b64 	%fd44, {%r3, %r10};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd6;
	}
	mov.b32 	%f2, %r11;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p2, %f1, 0f4086232B;
	@%p2 bra 	$L__BB4_4;

	setp.gt.f64 	%p3, %fd1, 0d8000000000000000;
	mov.f64 	%fd38, 0d7FF0000000000000;
	sub.f64 	%fd39, %fd38, %fd1;
	selp.f64 	%fd44, 0d0000000000000000, %fd39, %p3;
	setp.geu.f32 	%p4, %f1, 0f40874800;
	@%p4 bra 	$L__BB4_4;

	shr.u32 	%r12, %r2, 31;
	add.s32 	%r13, %r2, %r12;
	shr.s32 	%r14, %r13, 1;
	shl.b32 	%r15, %r14, 20;
	add.s32 	%r16, %r4, %r15;
	mov.b64 	%fd40, {%r3, %r16};
	sub.s32 	%r17, %r2, %r14;
	shl.b32 	%r18, %r17, 20;
	add.s32 	%r19, %r18, 1072693248;
	mov.u32 	%r20, 0;
	mov.b64 	%fd41, {%r20, %r19};
	mul.f64 	%fd44, %fd40, %fd41;

$L__BB4_4:
	add.f64 	%fd42, %fd44, 0d3FF0000000000000;
	rcp.rn.f64 	%fd43, %fd42;
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd43;

$L__BB4_5:
	ret;

}
	// .globl	hyperbolic_tangent_f64
.visible .entry hyperbolic_tangent_f64(
	.param .u64 hyperbolic_tangent_f64_param_0,
	.param .u64 hyperbolic_tangent_f64_param_1,
	.param .u32 hyperbolic_tangent_f64_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<72>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [hyperbolic_tangent_f64_param_0];
	ld.param.u64 	%rd3, [hyperbolic_tangent_f64_param_1];
	ld.param.u32 	%r4, [hyperbolic_tangent_f64_param_2];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	setp.ge.s32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB5_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r8, %r3};
	setp.ltu.f64 	%p2, %fd2, 0d3FE4F92224DD2F1A;
	@%p2 bra 	$L__BB5_3;
	bra.uni 	$L__BB5_2;

$L__BB5_3:
	mul.f64 	%fd47, %fd1, %fd1;
	mov.f64 	%fd48, 0d3F14359F420AFC3D;
	mov.f64 	%fd49, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd50, %fd49, %fd47, %fd48;
	mov.f64 	%fd51, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd52, %fd50, %fd47, %fd51;
	mov.f64 	%fd53, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd54, %fd52, %fd47, %fd53;
	mov.f64 	%fd55, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd56, %fd54, %fd47, %fd55;
	mov.f64 	%fd57, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd58, %fd56, %fd47, %fd57;
	mov.f64 	%fd59, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd60, %fd58, %fd47, %fd59;
	mov.f64 	%fd61, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd62, %fd60, %fd47, %fd61;
	mov.f64 	%fd63, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd64, %fd62, %fd47, %fd63;
	mov.f64 	%fd65, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd66, %fd64, %fd47, %fd65;
	mov.f64 	%fd67, 0dBFD5555555555550;
	fma.rn.f64 	%fd68, %fd66, %fd47, %fd67;
	mov.f64 	%fd69, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd68, %fd47, %fd69;
	fma.rn.f64 	%fd71, %fd70, %fd1, %fd1;
	bra.uni 	$L__BB5_4;

$L__BB5_2:
	add.f64 	%fd6, %fd2, %fd2;
	mov.f64 	%fd7, 0d4000000000000000;
	cvt.rn.f32.f64 	%f1, %fd6;
	mul.f32 	%f2, %f1, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f3, %f2;
	cvt.f64.f32 	%fd8, %f3;
	neg.f64 	%fd9, %fd8;
	mov.f64 	%fd10, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd11, %fd9, %fd10, %fd6;
	mov.f64 	%fd12, 0d3E928A27F89B6999;
	mov.f64 	%fd13, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd14, %fd13, %fd11, %fd12;
	mov.f64 	%fd15, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd16, %fd14, %fd11, %fd15;
	mov.f64 	%fd17, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd18, %fd16, %fd11, %fd17;
	mov.f64 	%fd19, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd20, %fd18, %fd11, %fd19;
	mov.f64 	%fd21, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd22, %fd20, %fd11, %fd21;
	mov.f64 	%fd23, 0d3F811111111173C4;
	fma.rn.f64 	%fd24, %fd22, %fd11, %fd23;
	mov.f64 	%fd25, 0d3FA555555555211A;
	fma.rn.f64 	%fd26, %fd24, %fd11, %fd25;
	mov.f64 	%fd27, 0d3FC5555555555540;
	fma.rn.f64 	%fd28, %fd26, %fd11, %fd27;
	mov.f64 	%fd29, 0d3FE0000000000005;
	fma.rn.f64 	%fd30, %fd28, %fd11, %fd29;
	mul.f64 	%fd31, %fd11, %fd30;
	fma.rn.f64 	%fd32, %fd31, %fd11, %fd11;
	ex2.approx.ftz.f32 	%f4, %f3;
	cvt.f64.f32 	%fd33, %f4;
	mov.f64 	%fd34, 0d3FF0000000000000;
	sub.f64 	%fd35, %fd34, %fd33;
	neg.f64 	%fd36, %fd32;
	fma.rn.f64 	%fd37, %fd36, %fd33, %fd35;
	sub.f64 	%fd38, %fd7, %fd37;
	rcp.approx.ftz.f64 	%fd39, %fd38;
	neg.f64 	%fd40, %fd38;
	fma.rn.f64 	%fd41, %fd40, %fd39, %fd34;
	fma.rn.f64 	%fd42, %fd41, %fd41, %fd41;
	fma.rn.f64 	%fd43, %fd42, %fd39, %fd39;
	neg.f64 	%fd44, %fd43;
	fma.rn.f64 	%fd45, %fd7, %fd44, %fd34;
	setp.gt.u32 	%p3, %r3, 1077088193;
	selp.f64 	%fd46, 0d3FF0000000000000, %fd45, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r9, %temp}, %fd46;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd46;
	}
	and.b32  	%r11, %r2, -2147483648;
	or.b32  	%r12, %r10, %r11;
	mov.b64 	%fd71, {%r9, %r12};

$L__BB5_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd71;

$L__BB5_5:
	ret;

}

